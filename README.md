# Azure Data Factory & Databricks End-to-End Pipeline (Medallion Architecture)

## ğŸ“Œ Overview
This project demonstrates an **end-to-end data engineering pipeline** using **Azure Data Factory (ADF)**, **Azure Databricks**, and **Azure Data Lake Storage Gen2 (ADLS)**.  
It implements the **Medallion Architecture** (Bronze â†’ Silver â†’ Gold) to ingest, process, and store trip transaction & ratings data for analytics.

![Project Architecture](images/project_architecture.png)

---

## ğŸš€ Use Case
- **Optimized Ride Analytics** â€“ Identify top-rated drivers, peak demand periods, and travel patterns.
- **Real-Time Data Processing & Alerts** â€“ Automated ingestion with notifications via Azure Logic Apps.
- **Structured Storage** â€“ Data stored in Delta tables for time travel, schema evolution, and efficient querying.

---

## ğŸ— Architecture
1. **Bronze Layer** â€“ Raw data from Azure SQL Database copied to ADLS via ADF.
2. **Silver Layer** â€“ Data cleaned, transformed, and structured in Databricks.
3. **Gold Layer** â€“ Aggregated, analytics-ready data stored in Delta format for BI/ML.

---

## ğŸ›  Services Used
- **Azure SQL Database** â€“ Source transactional & ratings data.
- **Azure Data Factory** â€“ Orchestrates data movement & transformations.
- **Azure Data Lake Storage Gen2** â€“ Stores raw & processed data.
- **Azure Databricks** â€“ Processes data with PySpark using Medallion Architecture.
- **Azure Logic Apps** â€“ Sends email alerts after pipeline completion.

---

## ğŸ“‚ Project Workflow
1. **Data Ingestion** â€“ ADF pipelines replicate SQL DB tables to ADLS Bronze folders.
2. **Transformation** â€“ Databricks notebooks process Bronze â†’ Silver â†’ Gold.
3. **Automation** â€“ ADF orchestrates notebook execution & triggers Logic App alerts.
4. **Storage** â€“ Delta tables stored in ADLS for analytics.

---

## ğŸ“Š Datasets
- **Trip Transactions** â€“ Trip details, timestamps, distance, and fares.
- **Ride Ratings** â€“ Driver ratings and customer satisfaction scores.

---

## âš™ï¸ Prerequisites
- Azure Subscription
- Azure SQL Database with sample data
- ADLS Gen2 Storage Account
- Databricks Workspace & Cluster
- Azure Data Factory instance
- Logic Apps for notifications

---

## ğŸ“œ Execution Steps
1. Create Azure Resource Group, SQL DB, ADLS Gen2, Databricks, and ADF.
2. Upload raw CSV data into **bronze** folder in ADLS.
3. Configure **ADF Dataflows** for Trip Transactions & Rewards Points.
4. Implement **Incremental Loads** using ADF filter expressions.
5. Build **Databricks Notebooks** for Bronze, Silver, and Gold processing.
6. Schedule and monitor pipelines in ADF.
7. Verify processed data in ADLS and Databricks tables.

---

## ğŸ† Key Features
- **Delta Lake Storage** with ACID transactions & schema enforcement.
- **Incremental Data Loads** for efficiency.
- **Automated Notifications** after pipeline runs.
- **Time Travel Queries** for historical data analysis.


